"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[161],{3032:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"tutorialSidebar":[{"type":"link","href":"/AI-Spec-Driven-Development-Book/docs/intro","label":"Introduction","docId":"intro","unlisted":false},{"type":"category","label":"Module 1: The Robotic Nervous System (ROS 2)","items":[{"type":"link","href":"/AI-Spec-Driven-Development-Book/docs/module1/","label":"Module 1: The Robotic Nervous System (ROS 2)","docId":"module1/index","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: The Digital Twin (Gazebo & Unity)","items":[{"type":"link","href":"/AI-Spec-Driven-Development-Book/docs/module2/","label":"Module 2: The Digital Twin (Gazebo & Unity)","docId":"module2/index","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: The AI-Robot Brain (NVIDIA Isaac)","items":[{"type":"link","href":"/AI-Spec-Driven-Development-Book/docs/module3/","label":"Module 3: The AI-Robot Brain (NVIDIA Isaac)","docId":"module3/index","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: Vision-Language-Action (VLA)","items":[{"type":"link","href":"/AI-Spec-Driven-Development-Book/docs/capstone/","label":"Module 4: Vision-Language-Action (VLA) - Capstone","docId":"capstone/index","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"link","href":"/AI-Spec-Driven-Development-Book/docs/conclusion","label":"Conclusion","docId":"conclusion","unlisted":false}]},"docs":{"capstone/implementation":{"id":"capstone/implementation","title":"Capstone: Vision-Language-Action (VLA) Agent Implementation","description":"This document details the implementation of the fully integrated autonomous humanoid AI agent that receives voice commands, plans tasks with an LLM, navigates, identifies objects, and manipulates them in simulation."},"capstone/index":{"id":"capstone/index","title":"Module 4: Vision-Language-Action (VLA) - Capstone","description":"The capstone module integrates all previous learning into a complete autonomous humanoid agent that receives voice commands, plans task sequences using LLMs, navigates obstacles, identifies objects using computer vision, and manipulates objects in simulation.","sidebar":"tutorialSidebar"},"conclusion":{"id":"conclusion","title":"Conclusion","description":"Congratulations on completing the Physical AI & Humanoid Robotics course! You now have the knowledge and tools to build sophisticated autonomous humanoid robots.","sidebar":"tutorialSidebar"},"intro":{"id":"intro","title":"Introduction","description":"Welcome to the Physical AI & Humanoid Robotics course. This comprehensive guide will take you through the essential concepts and practical implementations needed to build autonomous humanoid robots.","sidebar":"tutorialSidebar"},"module1/implementation":{"id":"module1/implementation","title":"Module 1: ROS 2 Control Pipeline Implementation","description":"This document details the implementation of the ROS 2 control pipeline for the humanoid robot."},"module1/index":{"id":"module1/index","title":"Module 1: The Robotic Nervous System (ROS 2)","description":"This module focuses on core middleware for robotic control, including ROS 2 nodes, publishers, subscribers, and services, connecting Python agents to ROS controllers via rclpy, and understanding and authoring humanoid URDF robot models.","sidebar":"tutorialSidebar"},"module2/implementation":{"id":"module2/implementation","title":"Module 2: Digital Twin Environment Implementation","description":"This document details the implementation of the digital twin environment for the humanoid robot, including physics simulation and sensor data."},"module2/index":{"id":"module2/index","title":"Module 2: The Digital Twin (Gazebo & Unity)","description":"This module covers simulation environments and physics engines, including physics simulation fundamentals in Gazebo, high-fidelity 3D environments in Unity for human-robot interaction, and sensor simulation for LiDAR, depth cameras, and IMUs.","sidebar":"tutorialSidebar"},"module3/implementation":{"id":"module3/implementation","title":"Module 3: AI-Robot Brain Implementation","description":"This document details the implementation of the AI-Robot brain for the humanoid robot, including perception pipelines and navigation using NVIDIA Isaac Sim, Isaac ROS, and Nav2."},"module3/index":{"id":"module3/index","title":"Module 3: The AI-Robot Brain (NVIDIA Isaac)","description":"This module focuses on advanced perception, navigation, and synthetic training using NVIDIA Isaac Sim for photorealistic simulation and dataset generation, Isaac ROS for GPU-accelerated VSLAM and perception, and Nav2 for bipedal humanoid locomotion and path planning.","sidebar":"tutorialSidebar"},"quickstart":{"id":"quickstart","title":"Quickstart Guide","description":"Get started with the Physical AI & Humanoid Robotics project in minutes."}}}}')}}]);