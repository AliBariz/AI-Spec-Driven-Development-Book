"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[852],{4280:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>c,default:()=>d,frontMatter:()=>a,metadata:()=>o,toc:()=>r});const o=JSON.parse('{"id":"capstone/index","title":"Module 4: Vision-Language-Action (VLA) - Capstone","description":"The capstone module integrates all previous learning into a complete autonomous humanoid agent that receives voice commands, plans task sequences using LLMs, navigates obstacles, identifies objects using computer vision, and manipulates objects in simulation.","source":"@site/docs/capstone/index.md","sourceDirName":"capstone","slug":"/capstone/","permalink":"/AI-Spec-Driven-Development-Book/docs/capstone/","draft":false,"unlisted":false,"editUrl":"https://github.com/AliBariz/AI-Spec-Driven-Development-Book/edit/main/docs/capstone/index.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Module 3: The AI-Robot Brain (NVIDIA Isaac)","permalink":"/AI-Spec-Driven-Development-Book/docs/module3/"},"next":{"title":"Conclusion","permalink":"/AI-Spec-Driven-Development-Book/docs/conclusion"}}');var t=i(4848),s=i(8453);const a={},c="Module 4: Vision-Language-Action (VLA) - Capstone",l={},r=[{value:"Topics Covered",id:"topics-covered",level:2},{value:"Capstone Project",id:"capstone-project",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2}];function u(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla---capstone",children:"Module 4: Vision-Language-Action (VLA) - Capstone"})}),"\n",(0,t.jsx)(n.p,{children:"The capstone module integrates all previous learning into a complete autonomous humanoid agent that receives voice commands, plans task sequences using LLMs, navigates obstacles, identifies objects using computer vision, and manipulates objects in simulation."}),"\n",(0,t.jsx)(n.h2,{id:"topics-covered",children:"Topics Covered"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Voice-to-Action systems using OpenAI Whisper"}),"\n",(0,t.jsx)(n.li,{children:"LLM-based cognitive planning: converting natural language instructions into ROS 2 action sequences"}),"\n",(0,t.jsx)(n.li,{children:"Full embodied task pipelines (perception \u2192 planning \u2192 control)"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"capstone-project",children:"Capstone Project"}),"\n",(0,t.jsx)(n.p,{children:"Build an autonomous humanoid agent that:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Receives a voice command"}),"\n",(0,t.jsx)(n.li,{children:"Plans a task sequence using an LLM"}),"\n",(0,t.jsx)(n.li,{children:"Navigates obstacles"}),"\n",(0,t.jsx)(n.li,{children:"Identifies an object using computer vision"}),"\n",(0,t.jsx)(n.li,{children:"Manipulates the object in simulation"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,t.jsx)(n.p,{children:"Students will be able to build a fully integrated humanoid AI agent demonstrating embodied intelligence."})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(u,{...e})}):u(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>c});var o=i(6540);const t={},s=o.createContext(t);function a(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);