"use strict";(globalThis.webpackChunkphysical_ai_humanoid_robotics=globalThis.webpackChunkphysical_ai_humanoid_robotics||[]).push([[251],{662:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"module2/implementation","title":"Module 2: Digital Twin Environment Implementation","description":"This document details the implementation of the digital twin environment for the humanoid robot, including physics simulation and sensor data.","source":"@site/docs/module2/implementation.md","sourceDirName":"module2","slug":"/module2/implementation","permalink":"/AI-Spec-Driven-Development-Book/docs/module2/implementation","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/module2/implementation.md","tags":[],"version":"current","frontMatter":{}}');var o=i(4848),t=i(8453);const r={},a="Module 2: Digital Twin Environment Implementation",l={},d=[{value:"Overview",id:"overview",level:2},{value:"Components",id:"components",level:2},{value:"Gazebo World Configuration",id:"gazebo-world-configuration",level:3},{value:"Sensor Simulation",id:"sensor-simulation",level:3},{value:"LiDAR Sensor",id:"lidar-sensor",level:4},{value:"Depth Camera",id:"depth-camera",level:4},{value:"IMU Sensor",id:"imu-sensor",level:4},{value:"URDF Integration",id:"urdf-integration",level:3},{value:"Usage",id:"usage",level:2},{value:"Launching the Digital Twin",id:"launching-the-digital-twin",level:3},{value:"Accessing Sensor Data",id:"accessing-sensor-data",level:3},{value:"Testing Sensor Data",id:"testing-sensor-data",level:3},{value:"Unity Environment (Conceptual)",id:"unity-environment-conceptual",level:2},{value:"Key Concepts Learned",id:"key-concepts-learned",level:2},{value:"Validation",id:"validation",level:2}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"module-2-digital-twin-environment-implementation",children:"Module 2: Digital Twin Environment Implementation"})}),"\n",(0,o.jsx)(n.p,{children:"This document details the implementation of the digital twin environment for the humanoid robot, including physics simulation and sensor data."}),"\n",(0,o.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,o.jsx)(n.p,{children:"The digital twin environment enables students to build realistic simulation environments for humanoid testing using Gazebo, incorporating physics simulation and sensor data."}),"\n",(0,o.jsx)(n.h2,{id:"components",children:"Components"}),"\n",(0,o.jsx)(n.h3,{id:"gazebo-world-configuration",children:"Gazebo World Configuration"}),"\n",(0,o.jsx)(n.p,{children:"The simulation environment is configured with a realistic world that includes:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Physics parameters (gravity, collision detection)"}),"\n",(0,o.jsx)(n.li,{children:"Environmental objects for testing"}),"\n",(0,o.jsx)(n.li,{children:"Proper lighting and visual elements"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"sensor-simulation",children:"Sensor Simulation"}),"\n",(0,o.jsx)(n.p,{children:"The digital twin includes three main types of sensors:"}),"\n",(0,o.jsx)(n.h4,{id:"lidar-sensor",children:"LiDAR Sensor"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Mounted on top of the robot's head"}),"\n",(0,o.jsx)(n.li,{children:"360-degree scanning capability"}),"\n",(0,o.jsx)(n.li,{children:"Range: 0.12m to 3.5m"}),"\n",(0,o.jsx)(n.li,{children:"64 ray samples for detailed environment mapping"}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"depth-camera",children:"Depth Camera"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Positioned on the front of the robot's head"}),"\n",(0,o.jsx)(n.li,{children:"640x480 resolution"}),"\n",(0,o.jsx)(n.li,{children:"60-degree horizontal field of view"}),"\n",(0,o.jsx)(n.li,{children:"Capable of generating both RGB and depth images"}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"imu-sensor",children:"IMU Sensor"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Located in the torso of the robot"}),"\n",(0,o.jsx)(n.li,{children:"Provides orientation and acceleration data"}),"\n",(0,o.jsx)(n.li,{children:"Includes realistic noise models for real-world simulation"}),"\n",(0,o.jsx)(n.li,{children:"100Hz update rate"}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"urdf-integration",children:"URDF Integration"}),"\n",(0,o.jsx)(n.p,{children:"The sensor configurations are integrated into the main humanoid URDF model using xacro macros:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"sensors/lidar.urdf.xacro"})," - LiDAR sensor macro"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"sensors/camera.urdf.xacro"})," - Depth camera macro"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.code,{children:"sensors/imu.urdf.xacro"})," - IMU sensor macro"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"usage",children:"Usage"}),"\n",(0,o.jsx)(n.h3,{id:"launching-the-digital-twin",children:"Launching the Digital Twin"}),"\n",(0,o.jsx)(n.p,{children:"To launch the robot with full sensor simulation in Gazebo:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Launch the robot in Gazebo with all sensors\nros2 launch humanoid_description gazebo.launch.py\n"})}),"\n",(0,o.jsx)(n.h3,{id:"accessing-sensor-data",children:"Accessing Sensor Data"}),"\n",(0,o.jsx)(n.p,{children:"The sensors publish data on the following topics:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:["LiDAR: ",(0,o.jsx)(n.code,{children:"/scan"})," (sensor_msgs/LaserScan)"]}),"\n",(0,o.jsxs)(n.li,{children:["Camera: ",(0,o.jsx)(n.code,{children:"/camera/depth/image_raw"})," (sensor_msgs/Image)"]}),"\n",(0,o.jsxs)(n.li,{children:["IMU: ",(0,o.jsx)(n.code,{children:"/imu/data"})," (sensor_msgs/Imu)"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"testing-sensor-data",children:"Testing Sensor Data"}),"\n",(0,o.jsx)(n.p,{children:"The system can be tested using the provided test script:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-bash",children:"# Run the sensor simulation test\npython3 ros_ws/src/humanoid_control/test/test_sensor_simulation.py\n"})}),"\n",(0,o.jsx)(n.h2,{id:"unity-environment-conceptual",children:"Unity Environment (Conceptual)"}),"\n",(0,o.jsx)(n.p,{children:"For human-robot interaction (HRI) scenarios requiring high-fidelity 3D visualization, Unity can be used as an alternative simulation environment. However, for academic purposes, we recommend using open-source alternatives such as:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Gazebo Garden"}),": For advanced physics simulation and rendering"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Ignition Gazebo"}),": For more realistic lighting and materials"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Webots"}),": For web-based simulation with good 3D visualization"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"These alternatives provide similar HRI capabilities while maintaining open-source accessibility for educational purposes."}),"\n",(0,o.jsx)(n.h2,{id:"key-concepts-learned",children:"Key Concepts Learned"}),"\n",(0,o.jsx)(n.p,{children:"Students will learn:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"How to configure realistic sensor simulation in Gazebo"}),"\n",(0,o.jsx)(n.li,{children:"How to integrate sensors into URDF models using xacro macros"}),"\n",(0,o.jsx)(n.li,{children:"How to access and process sensor data streams"}),"\n",(0,o.jsx)(n.li,{children:"How to validate that sensors are producing realistic data"}),"\n",(0,o.jsx)(n.li,{children:"How to troubleshoot sensor configurations"}),"\n",(0,o.jsx)(n.li,{children:"How to choose appropriate simulation environments for different use cases"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"validation",children:"Validation"}),"\n",(0,o.jsx)(n.p,{children:"The sensor simulation can be validated by:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Confirming that all sensor topics are publishing data"}),"\n",(0,o.jsx)(n.li,{children:"Verifying that data ranges are within expected bounds"}),"\n",(0,o.jsx)(n.li,{children:"Checking that sensors respond appropriately to environment changes"}),"\n",(0,o.jsx)(n.li,{children:"Testing that noise models provide realistic data variation"}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var s=i(6540);const o={},t=s.createContext(o);function r(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);